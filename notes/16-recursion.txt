Introduction
Recursion is the idea that a function calls itself. That is all there is to it. It‚Äôs used to take a big problem and start breaking it down into smaller and smaller pieces (‚ÄúDivide and Conquer‚Äù) and continuing to feed their solutions back into the original function until some sort of answer is achieved and the whole chain unwinds.

From the Wikipedia entry on Divide and Conquer Algorithms:

In computer science, divide and conquer (D&C) is an important algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

There‚Äôs also a right and wrong way to use recursion. The fact is, any problem you can solve recursively, you can also solve using the iterators that you know and love. If you find yourself saying ‚Äúwhy didn‚Äôt I just use a while loop here?‚Äù then you probably should have. You won‚Äôt often end up using a recursive solution to a problem, but you should get a feel for when it might be a good idea. Some problems also break down into far too many pieces and totally overwhelm your computer‚Äôs memory. There‚Äôs a balance.

In this brief lesson, you‚Äôll get a chance to learn more about when and how to use recursion and then in the next project you will get the chance to apply some of that (since it probably won‚Äôt really stick until you‚Äôve had a chance to try it).

Lesson overview
This section contains a general overview of topics that you will learn in this lesson.

Why is recursion a useful technique for solving a big problem?
What are the limitations of using recursive solutions?
What types of problems are more suited for loops than recursion?
What is meant by ‚Äúrecursive depth‚Äù?
What is a ‚Äústack overflow‚Äù (the concept, not the website)?
Why is that relevant to a recursive problem?

Recursion and stack
Let‚Äôs return to functions and study them more in-depth.

Our first topic will be recursion.

If you are not new to programming, then it is probably familiar and you could skip this chapter.

Recursion is a programming pattern that is useful in situations when a task can be naturally split into several tasks of the same kind, but simpler. Or when a task can be simplified into an easy action plus a simpler variant of the same task. Or, as we‚Äôll see soon, to deal with certain data structures.

When a function solves a task, in the process it can call many other functions. A partial case of this is when a function calls itself. That‚Äôs called recursion.

Two ways of thinking
For something simple to start with ‚Äì let‚Äôs write a function pow(x, n) that raises x to a natural power of n. In other words, multiplies x by itself n times.

pow(2, 2) = 4
pow(2, 3) = 8
pow(2, 4) = 16
There are two ways to implement it.

Iterative thinking: the for loop:

function pow(x, n) {
  let result = 1;

  // multiply result by x n times in the loop
  for (let i = 0; i < n; i++) {
    result *= x;
  }

  return result;
}

alert( pow(2, 3) ); // 8

Recursive thinking: simplify the task and call self:

function pow(x, n) {
  if (n == 1) {
    return x;
  } else {
    return x * pow(x, n - 1);
  }
}

alert( pow(2, 3) ); // 8
Please note how the recursive variant is fundamentally different.

When pow(x, n) is called, the execution splits into two branches:

              if n==1  = x
             /
pow(x, n) =
             \
              else     = x * pow(x, n - 1)
If n == 1, then everything is trivial. It is called the base of recursion, because it immediately produces the obvious result: pow(x, 1) equals x.
Otherwise, we can represent pow(x, n) as x * pow(x, n - 1). In maths, one would write xn = x * xn-1. This is called a recursive step: we transform the task into a simpler action (multiplication by x) and a simpler call of the same task (pow with lower n). Next steps simplify it further and further until n reaches 1.
We can also say that pow recursively calls itself till n == 1.
For example, to calculate pow(2, 4) the recursive variant does these steps:

pow(2, 4) = 2 * pow(2, 3)
pow(2, 3) = 2 * pow(2, 2)
pow(2, 2) = 2 * pow(2, 1)
pow(2, 1) = 2
So, the recursion reduces a function call to a simpler one, and then ‚Äì to even more simpler, and so on, until the result becomes obvious.

Recursion is usually shorter
A recursive solution is usually shorter than an iterative one.

Here we can rewrite the same using the conditional operator ? instead of if to make pow(x, n) more terse and still very readable:

function pow(x, n) {
  return (n == 1) ? x : (x * pow(x, n - 1));
}

The maximal number of nested calls (including the first one) is called recursion depth. In our case, it will be exactly n.

The maximal recursion depth is limited by JavaScript engine. We can rely on it being 10000, some engines allow more, but 100000 is probably out of limit for the majority of them. There are automatic optimizations that help alleviate this (‚Äútail calls optimizations‚Äù), but they are not yet supported everywhere and work only in simple cases.

That limits the application of recursion, but it still remains very wide. There are many tasks where recursive way of thinking gives simpler code, easier to maintain.

The execution context and stack
Now let‚Äôs examine how recursive calls work. For that we‚Äôll look under the hood of functions.

The information about the process of execution of a running function is stored in its execution context.

The execution context is an internal data structure that contains details about the execution of a function: where the control flow is now, the current variables, the value of this (we don‚Äôt use it here) and few other internal details.

One function call has exactly one execution context associated with it.

When a function makes a nested call, the following happens:

The current function is paused.
The execution context associated with it is remembered in a special data structure called execution context stack.
The nested call executes.
After it ends, the old execution context is retrieved from the stack, and the outer function is resumed from where it stopped.

pow(2, 3)
In the beginning of the call pow(2, 3) the execution context will store variables: x = 2, n = 3, the execution flow is at line 1 of the function.

We can sketch it as:

Context: { x: 2, n: 3, at line 1 } pow(2, 3)
That‚Äôs when the function starts to execute. The condition n == 1 is falsy, so the flow continues into the second branch of if:

function pow(x, n) {
  if (n == 1) {
    return x;
  } else {
    return x * pow(x, n - 1);
  }
}

alert( pow(2, 3) );

pow(2, 2)
To do a nested call, JavaScript remembers the current execution context in the execution context stack.

Here we call the same function pow, but it absolutely doesn‚Äôt matter. The process is the same for all functions:

The current context is ‚Äúremembered‚Äù on top of the stack.
The new context is created for the subcall.
When the subcall is finished ‚Äì the previous context is popped from the stack, and its execution continues.

Here‚Äôs the context stack when we entered the subcall pow(2, 2):

Context: { x: 2, n: 2, at line 1 } pow(2, 2)
Context: { x: 2, n: 3, at line 5 } pow(2, 3)
The new current execution context is on top (and bold), and previous remembered contexts are below.

When we finish the subcall ‚Äì it is easy to resume the previous context, because it keeps both variables and the exact place of the code where it stopped.

Please note:
Here in the picture we use the word ‚Äúline‚Äù, as in our example there‚Äôs only one subcall in line, but generally a single line of code may contain multiple subcalls, like pow(‚Ä¶) + pow(‚Ä¶) + somethingElse(‚Ä¶).

So it would be more precise to say that the execution resumes ‚Äúimmediately after the subcall‚Äù.

pow(2, 1)
The process repeats: a new subcall is made at line 5, now with arguments x=2, n=1.

A new execution context is created, the previous one is pushed on top of the stack:

Context: { x: 2, n: 1, at line 1 } pow(2, 1)
Context: { x: 2, n: 2, at line 5 } pow(2, 2)
Context: { x: 2, n: 3, at line 5 } pow(2, 3)

The exit
During the execution of pow(2, 1), unlike before, the condition n == 1 is truthy, so the first branch of if works:

function pow(x, n) {
  if (n == 1) {
    return x;
  } else {
    return x * pow(x, n - 1);
  }
}

There are no more nested calls, so the function finishes, returning 2.

As the function finishes, its execution context is not needed anymore, so it‚Äôs removed from the memory. The previous one is restored off the top of the stack:

Context: { x: 2, n: 2, at line 5 } pow(2, 2)
Context: { x: 2, n: 3, at line 5 } pow(2, 3)
The execution of pow(2, 2) is resumed. It has the result of the subcall pow(2, 1), so it also can finish the evaluation of x * pow(x, n - 1), returning 4.

Then the previous context is restored:

Context: { x: 2, n: 3, at line 5 } pow(2, 3)

The recursion depth in this case was: 3.

As we can see from the illustrations above, recursion depth equals the maximal number of context in the stack.

Note the memory requirements. Contexts take memory. In our case, raising to the power of n actually requires the memory for n contexts, for all lower values of n.

A loop-based algorithm is more memory-saving:

function pow(x, n) {
  let result = 1;

  for (let i = 0; i < n; i++) {
    result *= x;
  }

  return result;
}

Recursive traversals
Another great application of the recursion is a recursive traversal.

Imagine, we have a company. The staff structure can be presented as an object:

let company = {
  sales: [{
    name: 'John',
    salary: 1000
  }, {
    name: 'Alice',
    salary: 1600
  }],

  development: {
    sites: [{
      name: 'Peter',
      salary: 2000
    }, {
      name: 'Alex',
      salary: 1800
    }],

    internals: [{
      name: 'Jack',
      salary: 1300
    }]
  }
};

In other words, a company has departments.

A department may have an array of staff. For instance, sales department has 2 employees: John and Alice.

Or a department may split into subdepartments, like development has two branches: sites and internals. Each of them has their own staff.

It is also possible that when a subdepartment grows, it divides into subsubdepartments (or teams).

For instance, the sites department in the future may be split into teams for siteA and siteB. And they, potentially, can split even more. That‚Äôs not on the picture, just something to have in mind.

An iterative approach is not easy, because the structure is not simple. The first idea may be to make a for loop over company with nested subloop over 1st level departments. But then we need more nested subloops to iterate over the staff in 2nd level departments like sites‚Ä¶ And then another subloop inside those for 3rd level departments that might appear in the future? If we put 3-4 nested subloops in the code to traverse a single object, it becomes rather ugly.

Let‚Äôs try recursion.

As we can see, when our function gets a department to sum, there are two possible cases:

Either it‚Äôs a ‚Äúsimple‚Äù department with an array of people ‚Äì then we can sum the salaries in a simple loop.
Or it‚Äôs an object with N subdepartments ‚Äì then we can make N recursive calls to get the sum for each of the subdeps and combine the results.

The 1st case is the base of recursion, the trivial case, when we get an array.

The 2nd case when we get an object is the recursive step. A complex task is split into subtasks for smaller departments. They may in turn split again, but sooner or later the split will finish at (1).

The algorithm is probably even easier to read from the code:

let company = { // the same object, compressed for brevity
  sales: [{name: 'John', salary: 1000}, {name: 'Alice', salary: 1600 }],
  development: {
    sites: [{name: 'Peter', salary: 2000}, {name: 'Alex', salary: 1800 }],
    internals: [{name: 'Jack', salary: 1300}]
  }
};

// The function to do the job
function sumSalaries(department) {
  if (Array.isArray(department)) { // case (1)
    return department.reduce((prev, current) => prev + current.salary, 0); // sum the array
  } else { // case (2)
    let sum = 0;
    for (let subdep of Object.values(department)) {
      sum += sumSalaries(subdep); // recursively call for subdepartments, sum the results
    }
    return sum;
  }
}

alert(sumSalaries(company)); // 7700
The code is short and easy to understand (hopefully?). That‚Äôs the power of recursion. It also works for any level of

We can easily see the principle: for an object {...} subcalls are made, while arrays [...] are the ‚Äúleaves‚Äù of the recursion tree, they give immediate result.

Note that the code uses smart features that we‚Äôve covered before:

Method arr.reduce explained in the chapter Array methods to get the sum of the array.
Loop for(val of Object.values(obj)) to iterate over object values: Object.values returns an array of them.


Recursive structures
A recursive (recursively-defined) data structure is a structure that replicates itself in parts.

We‚Äôve just seen it in the example of a company structure above.

A company department is:

Either an array of people.
Or an object with departments.
For web-developers there are much better-known examples: HTML and XML documents.

In the HTML document, an HTML-tag may contain a list of:

Text pieces.
HTML-comments.
Other HTML-tags (that in turn may contain text pieces/comments or other tags etc).
That‚Äôs once again a recursive definition.

For better understanding, we‚Äôll cover one more recursive structure named ‚ÄúLinked list‚Äù that might be a better alternative for arrays in some cases.

Linked list
Imagine, we want to store an ordered list of objects.

The natural choice would be an array:

let arr = [obj1, obj2, obj3];
‚Ä¶But there‚Äôs a problem with arrays. The ‚Äúdelete element‚Äù and ‚Äúinsert element‚Äù operations are expensive. For instance, arr.unshift(obj) operation has to renumber all elements to make room for a new obj, and if the array is big, it takes time. Same with arr.shift().

The only structural modifications that do not require mass-renumbering are those that operate with the end of array: arr.push/pop. So an array can be quite slow for big queues, when we have to work with the beginning.

Alternatively, if we really need fast insertion/deletion, we can choose another data structure called a linked list.

The linked list element is recursively defined as an object with:

value.
next property referencing the next linked list element or null if that‚Äôs the end.

let list = {
  value: 1,
  next: {
    value: 2,
    next: {
      value: 3,
      next: {
        value: 4,
        next: null
      }
    }
  }
};

let list = { value: 1 };
list.next = { value: 2 };
list.next.next = { value: 3 };
list.next.next.next = { value: 4 };
list.next.next.next.next = null;

Here we can even more clearly see that there are multiple objects, each one has the value and next pointing to the neighbour. The list variable is the first object in the chain, so following next pointers from it we can reach any element.

The list can be easily split into multiple parts and later joined back:

let secondList = list.next.next;
list.next.next = null;

list.next.next = secondList;
And surely we can insert or remove items in any place.

For instance, to prepend a new value, we need to update the head of the list:

let list = { value: 1 };
list.next = { value: 2 };
list.next.next = { value: 3 };
list.next.next.next = { value: 4 };

// prepend the new value to the list
list = { value: "new item", next: list };

To remove a value from the middle, change next of the previous one:

list.next = list.next.next;

We made list.next jump over 1 to value 2. The value 1 is now excluded from the chain. If it‚Äôs not stored anywhere else, it will be automatically removed from the memory.

Unlike arrays, there‚Äôs no mass-renumbering, we can easily rearrange elements.

Naturally, lists are not always better than arrays. Otherwise everyone would use only lists.

The main drawback is that we can‚Äôt easily access an element by its number. In an array that‚Äôs easy: arr[n] is a direct reference. But in the list we need to start from the first item and go next N times to get the Nth element.

‚Ä¶But we don‚Äôt always need such operations. For instance, when we need a queue or even a deque ‚Äì the ordered structure that must allow very fast adding/removing elements from both ends, but access to its middle is not needed.

Lists can be enhanced:

We can add property prev in addition to next to reference the previous element, to move back easily.
We can also add a variable named tail referencing the last element of the list (and update it when adding/removing elements from the end).
‚Ä¶The data structure may vary according to our needs.

Terms:

Recursion is a programming term that means calling a function from itself. Recursive functions can be used to solve tasks in elegant ways.

When a function calls itself, that‚Äôs called a recursion step. The basis of recursion is function arguments that make the task so simple that the function does not make further calls.

A recursively-defined data structure is a data structure that can be defined using itself.

For instance, the linked list can be defined as a data structure consisting of an object referencing a list (or null).

list = { value, next -> list }
Trees like HTML elements tree or the department tree from this chapter are also naturally recursive: they have branches and every branch can have other branches.

Recursive functions can be used to walk them as we‚Äôve seen in the sumSalary example.

Any recursive function can be rewritten into an iterative one. And that‚Äôs sometimes required to optimize stuff. But for many tasks a recursive solution is fast enough and easier to write
Recursion
Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.

Explicit stack
Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue. This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications ‚Äî e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.

Stack size
In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise, the execution may fail because of stack overflow. D&C algorithms that are time-efficient often have relatively small recursion depth. For example, the quicksort algorithm can be implemented so that it never requires more than 
log
2
‚Å°
ùëõ
{\displaystyle \log _{2}n} nested recursive calls to sort 
ùëõ
{\displaystyle n} items.

Stack overflow may be difficult to avoid when using recursive procedures since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it. Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure. Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.

Choosing the base cases
In any recursive algorithm, there is considerable freedom in the choice of the base cases, the small subproblems that are solved directly in order to terminate the recursion.

Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve. For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples, there is only one base case to consider, and it requires no processing.

On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case, whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.

Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small. Note that, if the empty list were the only base case, sorting a list with 
ùëõ
{\displaystyle n} entries would entail maximally 
ùëõ
{\displaystyle n} quicksort calls that would do nothing but return immediately. Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.

Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation). For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.[11] Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.[11]

The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.[12]

Dynamic programming for overlapping subproblems
For some problems, the branched recursion may end up evaluating the same sub-problem many times over. In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique which is commonly known as memoization. Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming.
Iteration as a special case of recursion
The first insight is that iteration is a special case of recursion.

        void do_loop () { do { ... } while (e); }
is equivalent to:
        void do_loop () { ... ; if (e) do_loop(); }
A compiler can recognize instances of this form of recursion and turn them into loops or simple jumps. E.g.:
        void do_loop () { start: ...; if (e) goto start; }
Notice that this optimization also removes the space overhead associated with function calls.

Tail Calls
The second insight concerns tail calls. A call is said to be a tail call if it is the last thing that needs to be executed in a particular invocation of the function where it occurs. For example:

        void zig (int n) { ... ; if (e) zag(n-1); }
The call to zag is a tail call because, if it happens - i.e. if (e) evaluates to true - then it is the last thing that needs to be executed in the current invocation of zig.
What it so special about tail calls? Simply this: if zag is the last thing to be executed in zig, surely zig won't need its local variables while zag is executing, and it won't need them after zag returns since there won't be anything left to do. Therefore, we can release the local space allocated to zig before calling zag.

Thus, tail recursive algorithms can be optimized to execute in constant space - a tail recursive algorithm is one where the recursive steps are all tail calls
The bad news is that often the most natural version of an algorithm is not tail recursive. Consider the factorial function:
        int factorial(int n)
        { return (n == 0) ? 1 : n * factorial(n-1); }
The recursive call to factorial is not tail recursive: the last thing that needs to be done is the multiplication, not the call. Therefore, factorial executes in space proportional to n (linear space).
The good news is that it is often not too difficult to turn a non tail-recursive algorithm into a tail-recursive one. Typically, this is done by adding extra parameters to the definition: these parameters serve to accumulate intermediate results.

For example, the definition of factorial can be augmented with an `accumulator':

        int factorial(int n,int accu)
        { return (n == 0) ? accu : factorial(n-1,n*accu); }
Or we can keep the same interface as before and use an auxiliary definition:
        int fact_aux (int n,int accu)
        { return (n == 0) ? accu : fact_aux(n-1,n*accu); }
        int factorial(int n) { return fact_aux(n,1); }
A modern optimizing compiler will turn this version into machine code equivalent to the iterative version.
How do we know that this second version is correct? We prove it by induction. You will notice that there is a strong connection between recursion and induction. They are really two aspects of the same fundamental idea.

Notice that, for n>0, fact_aux(n,a) = fact_aux(n-1,n*a). On the right-hand side of the equation, the first argument has decreased by 1. As long as n is sufficiently large, we can iterate the process:

fact_aux(n,a) = fact_aux(n-1,n*a)
              = fact_aux(n-2,(n-1)*n*a)
              = fact_aux(n-3,(n-2)*(n-1)*n*a)
             ...
              = fact_aux(n-k,(n-k+1)*...*(n-2)*(n-1)*n*a)
in particular for k=n, we have:
fact_aux(n,a) = fact_aux(0,1*2*...*(n-2)*(n-1)*n*a)
              = 1*2*...*(n-2)*(n-1)*n*a
because when its 1st argument is 0, fact_aux simply returns its 2nd argument. By definition of factorial:
factorial(n) = fact_aux(n,1) = 1*2*...*(n-2)*(n-1)*n*1
This result is precisely `n!'.

The fibonacci function is defined by the following equations:

fib(0) = 1
fib(1) = 1
fib(n) = fib(n-2) + fib(n-1), if n>1
which we can directly implement by:
int fib(int n)
{ return (n == 0 || n == 1) ? 1 : fib(n-2)+fib(n-1); }
Unfortunately, there are two sources of inefficiency. Firstly, this algorithm is not tail recursive. Secondly, it spends a lot of time recomputing the same values over and over again. To wit, in order to compute fib(n):

First we compute fib(n-2), which in particular involves computing fib(n-3).
Then we compute fib(n-1), which requires recomputing both fib(n-2) and fib(n-3).
We can improve the algorithm as follows: we notice that the computation of fib(n-2) involves computing fib(n-3); therefore, if we could only save these two results, we could subsequently just add them together to produce fib(n-1). This the basis for our first optimization.
We are going to introduce the auxiliary function fib2 which is exactly like fib, but returns a compound value containing the two aforementioned results: i.e. fib2(n) contains both fib(n) and fib(n-1). Then, we shall write the function fib1 which computes the same value as fib, but does it more efficiently by calling fib2.

        typedef struct { int first,second; } Pair;
        Pair fib2 (int n) {
          if (n == 0) { Pair p = {1,0}; return p; }
          else {
            Pair p1 = fib2(n-1);
            Pair p2;

            p2.first  = p1.first + p1.second;
            p2.second = p1.first;

            return p2;
          }
        }
        int fib1 (int n) { return fib2(n).first; }

        How do we know this code is correct? First we verify the two base cases:
fib1(0) = 1 = fib(0)
fib1(1) = 1 = fib(1)
Then we proceed by induction, and show that for n>1
fib2(n) = {fib(n),fib(n-1)}
therefore: fib1(n) = fib2(n).first = fib(n)

The above improvement no longer spends time recomputing the same values. However, it is not tail recursive and consequently consumes stack space. It is possible to do better by using a bottom-up algorithm instead of a top-down algorithm. This time, we need to introduce 2 accumulators - they correspond to the pair of values of our first improvement.

        int fib3(int n,int i,int j) { return (n==0)?i:fib3(n-1,i+j,i); }
        int fib1(int n) { return fib3(n,1,0); }
How do we know this code is correct? Again, we proceed by induction:
fib1(n) = fib3(n,1,0) = fib3(n-k,fib(k),fib(k-1))
in particular for k=n: fib1(n) = fib3(0,fib(n),fib(n-1)) = fib(n)
You may convince yourself that the recursive algorithm above is essentially equivalent to the following iterative version:

        int fib1(int n)
        { int fib, fib_prev, fib_next, i;

          for (fib=1, fib_prev=0, i=0;    i<n;
               fib_next = fib+fib_prev,
               fib_prev = fib,
               fib      = fib_next,       i++);


               Introduction to the JavaScript recursive functions
A recursive function is a function that calls itself until it doesn‚Äôt. This technique is called recursion.

Suppose that you have a function called recurse(). The recurse() is a recursive function if it calls itself inside its body, like this:

function recurse() {
    // ...
    recurse();
    // ...
}
Code language: JavaScript (javascript)


A recursive function always has a condition to stop calling itself. Otherwise, it will call itself indefinitely. So a recursive function typically looks like the following:

function recurse() {
    if(condition) {
        // stop calling itself
        //...
    } else {
        recurse();
    }
}

JavaScript recursive function examples
Let‚Äôs take some examples of using recursive functions.

1) A simple JavaScript recursive function example
Suppose that you need to develop a function that counts down from a specified number to 1. For example, to count down from 3 to 1:

3
2
1
The following shows the countDown() function:

function countDown(fromNumber) {
    console.log(fromNumber);
}

countDown(3);

This countDown(3) shows only the number 3.

To count down from the number 3 to 1, you can:

show the number 3.
and call the countDown(2) that shows the number 2.
and call the countDown(1) that shows the number 1.
The following changes the countDown() to a recursive function:

function countDown(fromNumber) {
    console.log(fromNumber);
    countDown(fromNumber-1);
}

countDown(3);

This countDown(3) will run until the call stack size is exceeded, like this:

Uncaught RangeError: Maximum call stack size exceeded.
Code language: JavaScript (javascript)
‚Ä¶ because it doesn‚Äôt have the condition to stop calling itself.

The countdown will stop when the next number is zero. Therefore, you add an if condition as follows:

function countDown(fromNumber) {
    console.log(fromNumber);

    let nextNumber = fromNumber - 1;

    if (nextNumber > 0) {
        countDown(nextNumber);
    }
}
countDown(3);


The countDown() seems to work as expected.

However, as mentioned in the Function type tutorial, the function‚Äôs name is a reference to the actual function object.

If the function name is set to null somewhere in the code, the recursive function will stop working.

For example, the following code will result in an error:

let newYearCountDown = countDown;
// somewhere in the code
countDown = null;
// the following function call will cause an error
newYearCountDown(10);


let countDown = function f(fromNumber) {
    console.log(fromNumber);

    let nextNumber = fromNumber - 1;

    if (nextNumber > 0) {
        f(nextNumber);
    }
}

let newYearCountDown = countDown;
countDown = null;
newYearCountDown(10);

2) Calculate the sum of n natural numbers example
Suppose you need to calculate the sum of natural numbers from 1 to n using the recursion technique. To do that, you need to define the sum() recursively as follows:

sum(n) = n + sum(n-1)
sum(n-1) = n - 1 + sum(n-2)
...
sum(1) = 1

Base Case:

The function starts with an ‚Äúif‚Äù statement that checks if n is less than or equal to 1.
If n is 1 or less, the function simply returns n. This is the base case, which serves as the stopping condition for the recursi

Recursive Case:

If the base case is not met (i.e., n is greater than 1), the function enters the block after the if statement.
The function returns the sum of n and the result of calling itself with the argument (n - 1). This is where the recursion happens.

How it Works:

For example, if you call sum(3), the function first checks if 3 is less than or equal to 1 (base case not met).
Since it‚Äôs not the base case, it calculates 3 + sum(2). Now, it calls itself with the argument 2.
In the next recursive call with sum(2), it calculates 2 + sum(1).
Again, in the next recursive call with sum(1), it reaches the base case and returns 1.
Now, the previous calls are resolved: 2 + 1 gives 3, and 3 + 3 gives the final result of 6.

Termination:

The recursion keeps happening, reducing the problem to smaller subproblems until it reaches the base case.
Once the base case is reached, the function starts to unwind, combining the results from each level of recursion until the final result is obtained.


Summary
A recursive function is a function that calls itself until it doesn‚Äôt
A recursive function always has a condition that stops the function from calling itself.

1	function calcFactorial(num) {
2	    if (num === 1) {
3	        return 1;
4	    }
5	    return num * calcFactorial(num - 1);
6	}
7	
8	calcFactorial(5);

Introduction
Use what you have learnt about recursion so far to tackle two classic problems that can leverage recursion: Fibonacci and Merge Sort.

Fibonacci
The Fibonacci Sequence, is a numerical sequence where each number is the sum of the two numbers before it. Eg. 0, 1, 1, 2, 3, 5, 8, 13 are the first eight digits in the sequence.

You should already be thinking that perhaps this can be solved iteratively rather than recursively, and you would be correct. Nevertheless generating the sequence recursively is an excellent way to better understand recursion.

You can watch this video from Khan Academy on recursive Fibonacci to understand it further.

Merge sort
A significant part of computer science is dedicated to sorting data. An algorithm which uses the ‚Äòdivide and conquer‚Äô approach of recursion is able to reduce a sorting problem to smaller and smaller sub-problems.

Merge sort is one such sorting algorithm, and can be much faster than other algorithms such as bubble sort on the right data sets. Essentially merge sort recurses through an array of unsorted data until it reaches its smallest sub-set, a single item. Of course an array with a single item is considered sorted. Merge sort then merges the single items back together in sorted order. Pretty clever!

Representation of the Fibonacci series in JavaScript
As we have seen in the introduction section, the Fibonacci series in JavaScript is nothing but a mathematical sequence in which the current element is the sum of its previous two elements.

Note: The first two terms of the Fibonacci series in JavaScript are 0 and 1. The first two terms, i.e., 0 and 1, are always fixed.

So, we can generate the Fibonacci series using the sum concept. The order of the Fibonacci series is :
0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, ...

As we can see that a term (say 
ùëõ
ùë°
‚Ñé
n 
th
  term) can be calculated using the last two terms. The 
ùëõ
ùë°
‚Ñé
n 
th
  term can be calculated using the last two terms i.e. 
(
ùëõ
‚àí
1
)
ùë°
‚Ñé
(n‚àí1) 
th
  and 
(
ùëõ
‚àí
2
)
ùë°
‚Ñé
(n‚àí2) 
th
  term.

We can formulate this sequence as:
ùêπ
(
ùëõ
)
=
ùêπ
(
ùëõ
‚àí
1
)
+
ùêπ
(
ùëõ
‚àí
2
)
F(n)=F(n‚àí1)+F(n‚àí2)
where two numbers are fixed i.e. F(0) = 0 and F(1) = 1.

Refer to the picture shown below for better visualization.

fib(1) = 0
fib(2) = 1
fib(3) = fib(2) + fib(1)
fib(4) = fib(3) + fib(2)
fib(5) = fib(4) + fib(3)
...
fib(n) = fib(n-1) + fib(n-2)

Steps to Find the Fibonacci Series of n Numbers
Now, that we have a good understanding of what is Fibonacci series in JavaScript, let us now learn about the steps to generate the Fibonacci series in JavaScript.

The flow chart of the Fibonacci Series can be represented as:

Step 1: Declare variables x, y, z, n, i. x, and y are storing the first two terms. n is the value of the term that needs to be calculated. z will store the sum of the previous two terms i.e. x and y. i is the number that will track the number of Fibonacci terms generated.

Step 2: Initialize variables as x = 1, y = 1, i = 2.

Step 3: Take the value of n from the user.

Step 4: Display the value of x and y.

Step 5: Repeat the process i.e., adding the previous two terms to generate the Fibonacci series until i > n. As:

z = x+ y
Print the value of the current term i.e. z.
x = y, y = z
i = i + 1
Step 6: Stop the process as i becomes equivalent to n. So, we have generated n terms of the Fibonacci series in JavaScript.

Get the Fibonacci series up to n terms using for loop
A loop like for and while can be used to generate the Fibonacci series. We can run a loop from 2 to N, and in each iteration, the sum of the previous two elements is calculated and stored.

So, at the end of the nth iteration, we will have the nth Fibonacci term in front of us.

Let us code the above approach using for loop. Suppose we want to generate the 
7
ùë°
‚Ñé
7 
th
  term of the Fibonacci series in JavaScript.

  function fibonacci(num) {
  // x is representing the first term,
  // y is representing the second term, and
  // z is representing the sum of x and y.
  var x = 0;
  var y = 1;
  var z;
  var i = 0;
  for (i = 2; i < num; i++) {
    z = x + y;
    x = y;
    y = z;
  }
  return y;
}

var num = 7;
answer = fibonacci(num);

console.log("The 7th term of the Fibonacci series is: ", answer);


Time and Space Complexities
Since we are not storing the vales, the printing of the nth term is taking O(1) time on the other hand, the loop is running for (n-2) times which is equivalent to O(n). So, the overall time complexity of the above approach is O(n).

The space complexity is O(1) as as we are not taking any extra space(except some variables that takes O(1) space).